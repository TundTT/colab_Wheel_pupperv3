{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM6Du54tb10P"
      },
      "source": [
        "# **Hello!**\n",
        "Welcome to the Pupper training grounds.\n",
        "\n",
        "Use this colab to train a RL controller (a \"policy\") for your Pupper!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MOdG86sD9gb"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "## Before running this colab\n",
        "1. Make a Weights and Biases account and get your API Key\n",
        "\n",
        "## Running this colab\n",
        "1. Change GPU to A100 or L4 by clicking \"Runtime\" then \"Change runtime type\"\n",
        "2. Run this colab by clicking \"Run all\" or running each cell individually.\n",
        "3. Pay attention to the cell below \"Log in to Weights and Biases\" which will prompt you for your API key.\n",
        "\n",
        "## Going deeper\n",
        "1. Increase the training time by setting the \"training_config.ppo.num_timesteps\" value to 1 billion and \"training_config.ppo.learning_rate\" to 1e-5. Training longer usually leads to better policies (higher reward), which are more stable, agile, etc. After changing a setting, make sure to \"Run all\" again so that config values are properly propagated.\n",
        "2. Add obstacles to the training environment by setting the \"training_config.n_obstacles\" variable. 5 to 100 is a reasonable range. I tend to use 20.\n",
        "3. Train a backflip! This is the wild west - it's all on you! See mujoco playground for some ideas how to implement this\n",
        "\n",
        "## Deploying to the robot\n",
        "See https://pupper-v3-documentation.readthedocs.io/en/latest/development/modifying_code.html\n",
        "\n",
        "## Trouble-shooting\n",
        "Cannot interrupt training run: Click \"Runtime\" -> \"Restart session and run all\"\n",
        "\n",
        "Get help on our Discord https://discord.com/invite/qbmaU8NmP2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWNkLJXDLw6m"
      },
      "source": [
        "# Log in to Weights and Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceWm771NPMum"
      },
      "source": [
        "I recommend you use colab \"secrets\" (see key icon on the left) and add a secret \"wandb_key\" with your Weights & Biases API key. If you choose not to, you can enter the API key in the cell with wandb.login when it prompts you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP34paxbOhnp"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "wandb_key = \"6dcdd3ca314158819262ba82676aa696b2598f43\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn2fSg1ILt_J"
      },
      "outputs": [],
      "source": [
        "!pip install -q wandb\n",
        "import wandb\n",
        "wandb.login(key=wandb_key if wandb_key else None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvyGCsgSCxHQ"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqo7pyX-n72M"
      },
      "outputs": [],
      "source": [
        "!pip install mujoco==3.2.7 mujoco-mjx==3.2.7 brax==0.12.1 flax==0.10.2 orbax==0.1.9\n",
        "!pip install black[jupyter] --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfKU2iMadHK_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n",
        "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
        "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
        "os.environ['XLA_FLAGS'] = xla_flags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLH3VvWjbP-Z"
      },
      "outputs": [],
      "source": [
        "# Clean up incompatible versions AND plugins\n",
        "!pip uninstall -y jax jaxlib orbax-checkpoint jax-cuda12-plugin jax-cuda12-pjrt\n",
        "\n",
        "# Install the LATEST JAX with CUDA support and compatible orbax\n",
        "# We use \"jax[cuda12]\" to get the latest GPU-enabled version automatically\n",
        "!pip install \"jax[cuda12]\" orbax-checkpoint -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbZxYDxzoz5R"
      },
      "outputs": [],
      "source": [
        "#@title Check if MuJoCo installation was successful\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import distutils.util\n",
        "import os\n",
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "    f.write(\"\"\"{\n",
        "    \"file_format_version\" : \"1.0.0\",\n",
        "    \"ICD\" : {\n",
        "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n",
        "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
        "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
        "os.environ['XLA_FLAGS'] = xla_flags\n",
        "\n",
        "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "print('Setting environment variable to use GPU rendering:')\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "try:\n",
        "  print('Checking that the installation succeeded:')\n",
        "  import mujoco\n",
        "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "print('Installation successful.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5f4w3Kq2X14"
      },
      "outputs": [],
      "source": [
        "#@title Import packages for plotting and creating graphics\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "from typing import Callable, NamedTuple, Optional, Union, List\n",
        "\n",
        "# Graphics and plotting.\n",
        "print('Installing mediapy:')\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# More legible printing from numpy.\n",
        "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObF1UXrkb0Nd"
      },
      "outputs": [],
      "source": [
        "#@title Import MuJoCo, MJX, and Brax\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import functools\n",
        "from IPython.display import HTML\n",
        "import jax\n",
        "from jax import numpy as jp\n",
        "import numpy as np\n",
        "from typing import Any, Dict, Sequence, Tuple, Union\n",
        "\n",
        "from brax import base\n",
        "from brax import envs\n",
        "from brax import math\n",
        "from brax.base import Base, Motion, Transform\n",
        "from brax.envs.base import Env, PipelineEnv, State\n",
        "from brax.mjx.base import State as MjxState\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "from brax.training.agents.ppo import networks as ppo_networks\n",
        "from brax.io import html, mjcf, model\n",
        "\n",
        "from etils import epath\n",
        "from flax import struct\n",
        "from matplotlib import pyplot as plt\n",
        "import mediapy as media\n",
        "from ml_collections import config_dict\n",
        "import mujoco\n",
        "from mujoco import mjx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y262K4V6mQWO"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7y2mHlcs-uc"
      },
      "source": [
        "## pupperv3-mjx repo config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RWANdG-pksC"
      },
      "outputs": [],
      "source": [
        "pupperv3_mjx_config = config_dict.ConfigDict()\n",
        "pupperv3_mjx_config.branch = \"main\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvhB8kedFkLi"
      },
      "outputs": [],
      "source": [
        "!rm -rf pupperv3_mjx\n",
        "!git clone -b {pupperv3_mjx_config.branch} https://github.com/TundTT/mjx_Wheels_pupperv3.git\n",
        "!mv mjx_Wheels_pupperv3 pupperv3_mjx\n",
        "!cd pupperv3_mjx && git pull && pip install -q ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nZRV6QEu2sk"
      },
      "source": [
        "## Simulation Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yqn-xUQGpg3"
      },
      "outputs": [],
      "source": [
        "# Pupper model configuration\n",
        "simulation_config = config_dict.ConfigDict()\n",
        "simulation_config.model_repo = 'https://github.com/TundTT/description_Wheels_pupperv3.git'\n",
        "simulation_config.model_branch = 'main'\n",
        "\n",
        "!rm -rf description_Wheels_pupperv3\n",
        "!git clone {simulation_config.model_repo} -b {simulation_config.model_branch}\n",
        "!cd description_Wheels_pupperv3 && git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlfOsZOf2BeB"
      },
      "outputs": [],
      "source": [
        "# Select model\n",
        "simulation_config.original_model_path = 'description_Wheels_pupperv3/description/mujoco_xml/Wheel_pupper.xml' # 3Nm max\n",
        "\n",
        "simulation_config.model_xml = epath.Path(simulation_config.original_model_path).read_text()\n",
        "simulation_config.model_path = \"description_Wheels_pupperv3/description/mujoco_xml/model_with_obstacles.xml\"\n",
        "\n",
        "# Model body names\n",
        "simulation_config.upper_leg_body_names = [\"leg_front_r_2\", \"leg_front_l_2\", \"leg_back_r_2\", \"leg_back_l_2\"]\n",
        "simulation_config.lower_leg_body_names = [\"Wheel_FR\", \"Wheel_FL\", \"Wheel_BR\", \"Wheel_BL\"]\n",
        "simulation_config.foot_site_names = [\n",
        "    \"leg_front_r_3_foot_site\",\n",
        "    \"leg_front_l_3_foot_site\",\n",
        "    \"leg_back_r_3_foot_site\",\n",
        "    \"leg_back_l_3_foot_site\",\n",
        "]\n",
        "simulation_config.torso_name = \"base_link\"\n",
        "\n",
        "# Foot radius\n",
        "simulation_config.foot_radius = 0.02\n",
        "\n",
        "# Collision detection\n",
        "simulation_config.max_contact_points = 20 # Default: 8. 20 recommended for non-flat terrain.\n",
        "simulation_config.max_geom_pairs = 20 # Default: 8. 20 recommended for non-flat terrain.\n",
        "\n",
        "# Joint limits\n",
        "sys_temp = mjcf.load(simulation_config.original_model_path)\n",
        "joint_upper_limits = np.array(sys_temp.jnt_range[1:, 1])\n",
        "joint_lower_limits = np.array(sys_temp.jnt_range[1:, 0])\n",
        "\n",
        "# Unlock Wheel limits (indices 2, 5, 8, 11)\n",
        "# Wheel_FR (2), Wheel_FL (5), Wheel_BR (8), Wheel_BL (11)\n",
        "wheel_indices = [2, 5, 8, 11]\n",
        "\n",
        "# Set limits to infinity for continuous rotation\n",
        "joint_upper_limits[wheel_indices] = float('inf')\n",
        "joint_lower_limits[wheel_indices] = float('-inf')\n",
        "\n",
        "# Update the configuration object\n",
        "simulation_config.joint_upper_limits = joint_upper_limits.tolist()\n",
        "simulation_config.joint_lower_limits = joint_lower_limits.tolist()\n",
        "\n",
        "# Physics timestep\n",
        "simulation_config.physics_dt = 0.004 # Physics dt [s]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1neH_vwu2Ku"
      },
      "source": [
        "## Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dctacKGKufq9"
      },
      "outputs": [],
      "source": [
        "from pupperv3_mjx import domain_randomization\n",
        "\n",
        "import importlib\n",
        "\n",
        "importlib.reload(domain_randomization)\n",
        "\n",
        "training_config = config_dict.ConfigDict()\n",
        "\n",
        "# Checkpoint\n",
        "# Will use checkpoint from W&B run \"blah-blah-{checkpoint_run_number}\"\n",
        "training_config.checkpoint_run_number = None\n",
        "\n",
        "# Environment timestep\n",
        "training_config.environment_dt = 0.02\n",
        "\n",
        "# PPO params\n",
        "training_config.ppo = config_dict.ConfigDict()\n",
        "training_config.ppo.num_timesteps = 300_000_000   # Default: 300M. Set to 1B for better policy.\n",
        "training_config.ppo.episode_length = 500         # Default: 500\n",
        "training_config.ppo.num_evals = 11                # Default: 10\n",
        "training_config.ppo.reward_scaling = 1            # Default: 1\n",
        "training_config.ppo.normalize_observations = True # Default: True\n",
        "training_config.ppo.action_repeat = 1             # Default: 1\n",
        "training_config.ppo.unroll_length = 20            # Default: 20\n",
        "training_config.ppo.num_minibatches = 32          # Default: 32\n",
        "training_config.ppo.num_updates_per_batch = 4     # Default: 4\n",
        "training_config.ppo.discounting = 0.97            # Default: 0.97\n",
        "training_config.ppo.learning_rate = 3.0e-4        # Default: 3.0e-4. Set to 3e-5 if using >300M timesteps.\n",
        "training_config.ppo.entropy_cost = 1e-2           # Default: 1e-2\n",
        "training_config.ppo.num_envs = 8192               # Default: 8192\n",
        "training_config.ppo.batch_size = 256              # Default: 256\n",
        "\n",
        "# Command sampling\n",
        "training_config.resample_velocity_step = training_config.ppo.episode_length // 2\n",
        "training_config.lin_vel_x_range = [-0.75, 0.75]  # min max [m/s]. Default: [-0.75, 0.75]\n",
        "training_config.lin_vel_y_range = [-0.5, 0.5]  # min max [m/s]. Default: [-0.5, 0.5]\n",
        "training_config.ang_vel_yaw_range = [-2.0, 2.0]  # min max [rad/s]. Default: [-2.0, 2.0]\n",
        "training_config.zero_command_probability = 0.02\n",
        "training_config.stand_still_command_threshold = 0.05\n",
        "\n",
        "# Orientation command sampling in degrees\n",
        "training_config.maximum_pitch_command = 0.0 # [radians]\n",
        "training_config.maximum_roll_command = 0.0 # [radians]\n",
        "\n",
        "# Desired body orientation\n",
        "training_config.desired_world_z_in_body_frame = (0.0, 0.0, 1.0) # Default: (0.0, 0.0, 1.0)\n",
        "\n",
        "# Termination\n",
        "# NOTE: without a body collision geometry, can't train recovery policy\n",
        "training_config.terminal_body_z = 0.05  # Episode ends if body center goes below this height [m] Default: 0.10 m\n",
        "training_config.terminal_body_angle = 0.70  # Episode ends if body angle relative to vertical is more than this. Default: 0.52 rad (30 deg)\n",
        "training_config.early_termination_step_threshold = training_config.ppo.episode_length // 2 # Default: 500\n",
        "\n",
        "# Joint PD overrides\n",
        "training_config.dof_damping = 0.25  # Joint damping [Nm / (rad/s)] Default: 0.25\n",
        "training_config.position_control_kp = 5.0  # Joint stiffness [Nm / rad] Default: 5.0\n",
        "\n",
        "# Default joint angles\n",
        "training_config.default_pose = jp.array(\n",
        "    [0.26, 0.0, 0, -0.26, 0.0, 0, 0.26, 0.0, 0, -0.26, 0.0, 0]\n",
        ")\n",
        "\n",
        "# 3 legs (BL up)\n",
        "# training_config.default_pose = jp.array(\n",
        "#     [0.26, 0.0, -0.52, -0.26, 0.0, 0.52, 0.26, 0.0, -0.52, -0.26, 0.0, -0.6]\n",
        "# )\n",
        "\n",
        "# Desired abduction angles\n",
        "training_config.desired_abduction_angles = jp.array(\n",
        "    [0.0, 0.0, 0.0, 0.0] #[-0.2, 0.2, -0.2, 0.2]\n",
        ")\n",
        "\n",
        "# Height field\n",
        "## Type of height field\n",
        "training_config.height_field_random = False\n",
        "training_config.height_field_steps = False\n",
        "### Steps type params\n",
        "training_config.height_field_step_size = 4\n",
        "## General height field settings\n",
        "training_config.height_field_grid_size = 256\n",
        "training_config.height_field_group = \"0\"\n",
        "training_config.height_field_radius_x = 10.0 # [m]\n",
        "training_config.height_field_radius_y = 10.0 # [m]\n",
        "training_config.height_field_elevation_z = 0.02 # [m]\n",
        "training_config.height_field_base_z = 0.2 # [m]\n",
        "\n",
        "# Domain randomization\n",
        "## Perturbations\n",
        "training_config.kick_probability = 0.04        # Kick the robot with this probability. Default: 0.04\n",
        "training_config.kick_vel = 0.10               # Change the torso velocity by up to this much in x and y direction [m/s] Default: 0.1\n",
        "training_config.angular_velocity_noise = 0.1  # Default: 0.1 [rad/s]\n",
        "training_config.gravity_noise = 0.05            # Default: 0.05 [u]\n",
        "training_config.motor_angle_noise = 0.05        # Default: 0.05 [rad]\n",
        "training_config.last_action_noise = 0.01       # Default: 0.01 [rad]\n",
        "\n",
        "## Motors\n",
        "training_config.position_control_kp_multiplier_range = (0.6, 1.1)\n",
        "training_config.position_control_kd_multiplier_range = (0.8, 1.5)\n",
        "\n",
        "## Starting position\n",
        "training_config.start_position_config = domain_randomization.StartPositionRandomization(\n",
        "    x_min=-2.0, x_max=2.0, y_min=-2.0, y_max=2.0, z_min=0.15, z_max=0.20\n",
        ")\n",
        "\n",
        "## Latency distribution\n",
        "# Action latency\n",
        "# 0 latency with 20% prob, 1 timestep latency with 80% prob\n",
        "training_config.latency_distribution = jp.array([0.2, 0.8])\n",
        "\n",
        "# IMU latency\n",
        "# 0 latency with 50% prob, 1 timestep latency with 50% prob\n",
        "training_config.imu_latency_distribution = jp.array([0.5, 0.5])\n",
        "\n",
        "## Body CoM\n",
        "training_config.body_com_x_shift_range = (-0.02, 0.03) # Default: -0.02, 0.02\n",
        "training_config.body_com_y_shift_range = (-0.005, 0.005)\n",
        "training_config.body_com_z_shift_range = (-0.005, 0.005)\n",
        "\n",
        "## Mass and inertia randomization for all bodies\n",
        "training_config.body_mass_scale_range = (0.9, 1.3)\n",
        "training_config.body_inertia_scale_range = (0.9, 1.3)\n",
        "\n",
        "## Friction\n",
        "training_config.friction_range = (0.6, 1.4)\n",
        "\n",
        "# Obstacles\n",
        "# Set to ~20 for a policy that can go over obstacles.\n",
        "# Rather than train a policy on obstacles from scratch, it is best to fine-tune\n",
        "# a policy trained on flat ground.\n",
        "# Set training_config.checkpoint_run_number above to fine-tune that policy (downloads weights and biases)\n",
        "training_config.n_obstacles = 0 #\n",
        "training_config.obstacle_x_range = (-3.0, 3.0)  # [m]\n",
        "training_config.obstacle_y_range = (-3.0, 3.0)  # [m]\n",
        "training_config.obstacle_height = 0.04  # [m]\n",
        "training_config.obstacle_length = 2.0  # [m]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwgi-va9DWdJ"
      },
      "source": [
        "## Policy config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPOeTIpIDXli"
      },
      "outputs": [],
      "source": [
        "policy_config = config_dict.ConfigDict()\n",
        "\n",
        "policy_config.use_imu = True # Whether to use IMU in policy. Default: True\n",
        "\n",
        "policy_config.observation_history = 4  # number of stacked observations to give the policy\n",
        "\n",
        "# 4 legs\n",
        "policy_config.action_scale = 0.75  # Default 0.75\n",
        "\n",
        "# 3 legs (BL up)\n",
        "# policy_config.action_scale = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.1] # no back left knee or front knee\n",
        "\n",
        "policy_config.hidden_layer_sizes = (256, 128, 128, 128)\n",
        "# Default for locomotion is (256, 128, 128, 128)\n",
        "\n",
        "# Can change value network with\n",
        "# policy_config.value_hidden_layer_sizes\n",
        "# The network defaults to (256, 256, 256, 256, 256)\n",
        "# https://github.com/google/brax/blob/01ca8cafd410a7b623e01628fae65e07065dcb0d/brax/training/agents/ppo/networks.py#L72\n",
        "\n",
        "# RTNeural supports relu, tanh, sigmoid (not great), softmax, elu, prelu\n",
        "# Swish was really good in terms of training but not supported in RTNeural rn\n",
        "policy_config.activation = \"elu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hrx6uBAxEW0"
      },
      "source": [
        "## Reward config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYq1SKJww5PY"
      },
      "outputs": [],
      "source": [
        "reward_config = config_dict.ConfigDict()\n",
        "reward_config.rewards = config_dict.ConfigDict()\n",
        "reward_config.rewards.scales = config_dict.ConfigDict()\n",
        "\n",
        "# Track linear velocity\n",
        "reward_config.rewards.scales.tracking_lin_vel = 1.5\n",
        "\n",
        "# Track the angular velocity along z-axis, i.e. yaw rate.\n",
        "reward_config.rewards.scales.tracking_ang_vel = 0.8\n",
        "\n",
        "# Track the given body orientation (desired world z axis in body frame)\n",
        "# Not working right nowkick\n",
        "reward_config.rewards.scales.tracking_orientation = 0.5 # 0.5 default\n",
        "\n",
        "# Below are regularization terms, we roughly divide the\n",
        "# terms to base state regularizations, joint\n",
        "# regularizations, and other behavior regularizations.\n",
        "# Penalize the base velocity in z direction, L2 penalty.\n",
        "reward_config.rewards.scales.lin_vel_z = -0.1  # -2.0\n",
        "\n",
        "# Penalize the base roll and pitch rate. L2 penalty.\n",
        "reward_config.rewards.scales.ang_vel_xy = -0.002  # -0.05,\n",
        "\n",
        "# Penalize non-zero roll and pitch angles. L2 penalty.\n",
        "reward_config.rewards.scales.orientation = -0.0  # -5.0\n",
        "\n",
        "# L2 regularization of joint torques, sum(|tau|^2).\n",
        "reward_config.rewards.scales.torques = -0.025  # Default -0.025\n",
        "\n",
        "# L2 regularization of joint accelerations sum(|qdd|^2)\n",
        "reward_config.rewards.scales.joint_acceleration = -1e-6 # [rad/s/s] Default: -1e-6\n",
        "\n",
        "# L1 regularization of mechanical work, |v * tau|.\n",
        "reward_config.rewards.scales.mechanical_work = 0  # -0.01,\n",
        "\n",
        "# Penalize the change in the action and encourage smooth\n",
        "# actions. L1 regularization |action - last_action|^2\n",
        "reward_config.rewards.scales.action_rate = -0.1  # Default: -0.1 originally -0.01\n",
        "\n",
        "# Encourage long swing steps. However, it does not\n",
        "# encourage high clearances.\n",
        "reward_config.rewards.scales.feet_air_time = 0.02 # 0.02  # originally 0.2\n",
        "\n",
        "# Encourage joints at default position at zero command, L1 regularization\n",
        "# |q - q_default|.\n",
        "reward_config.rewards.scales.stand_still = -0.00  # -0.5\n",
        "\n",
        "# Encourage zero joint velocity at zero command, L1 regularization\n",
        "# |q_dot|.\n",
        "# Activates when norm(command) < stand_still_command_threshold\n",
        "# Commands below this threshold are sampled with probability zero_command_probability\n",
        "reward_config.rewards.scales.stand_still_joint_velocity = -0.2  # -0.05 was not enough with 1%\n",
        "\n",
        "# Encourage zero abduction angle so legs don't spread so far out\n",
        "# L2 loss on ||abduction_motors - desired||^2\n",
        "reward_config.rewards.scales.abduction_angle = -0.01 #-0.05 # -0.2 is too much, -0.02 produced weird turning, -0.005 is little\n",
        "\n",
        "# Early termination penalty.\n",
        "reward_config.rewards.scales.termination = -100.0\n",
        "\n",
        "# Penalizing foot slipping on the ground.\n",
        "reward_config.rewards.scales.foot_slip = -0.2\n",
        "\n",
        "# Penalize knees hitting the ground\n",
        "reward_config.rewards.scales.knee_collision = -10.0\n",
        "\n",
        "# Penalize body hitting ground\n",
        "reward_config.rewards.scales.body_collision = -0.5\n",
        "\n",
        "# Tracking reward = exp(-error^2/sigma).\n",
        "reward_config.rewards.tracking_sigma = 0.25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-gxymQh6gQg"
      },
      "source": [
        "##Export config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bqw0rOF6jGd"
      },
      "outputs": [],
      "source": [
        "# Define the path to the subdirectory\n",
        "export_config = config_dict.ConfigDict()\n",
        "export_config.gdrive_save_dir = '/content/drive/MyDrive/pupper_policies'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncR52Hvl3ZOn"
      },
      "source": [
        "## Github repo config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LIhbybr3Yp2"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "def get_hash():\n",
        "  return subprocess.check_output(['git', 'rev-parse', 'HEAD']).strip().decode('utf-8')\n",
        "\n",
        "repo_config = config_dict.ConfigDict()\n",
        "%cd /content/pupperv3_mjx\n",
        "repo_config.pupperv3_mjx_hash = get_hash()\n",
        "%cd /content/description_Wheels_pupperv3\n",
        "repo_config.description_Wheels_pupperv3_hash = get_hash()\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi69q6o5X8qC"
      },
      "source": [
        "## Create aggregated config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xE_9GLezQ6bF"
      },
      "outputs": [],
      "source": [
        "temp_config = config_dict.ConfigDict()\n",
        "temp_config.simulation = simulation_config\n",
        "temp_config.training = training_config\n",
        "temp_config.policy = policy_config\n",
        "temp_config.reward = reward_config\n",
        "temp_config.export = export_config\n",
        "temp_config.repo = repo_config\n",
        "temp_config.pupperv3_mjx = pupperv3_mjx_config\n",
        "\n",
        "CONFIG = config_dict.FrozenConfigDict(temp_config)\n",
        "\n",
        "# Prevent user from accidentally making changes to these configs which are not used\n",
        "del temp_config, reward_config, policy_config, training_config, simulation_config, export_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrkA-Nhi1ora"
      },
      "source": [
        "# Modify robot model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iKn6sSo7kSy"
      },
      "source": [
        "##Set contact options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kagHlf-9hiS"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "xml_str = epath.Path(CONFIG.simulation.original_model_path).read_text()\n",
        "tree = ET.ElementTree(ET.fromstring(xml_str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9VHtcfo-00v"
      },
      "outputs": [],
      "source": [
        "from pupperv3_mjx import utils\n",
        "import importlib\n",
        "importlib.reload(utils)\n",
        "\n",
        "tree = utils.set_mjx_custom_options(tree,\n",
        "                                    max_contact_points=CONFIG.simulation.max_contact_points,\n",
        "                                    max_geom_pairs=CONFIG.simulation.max_geom_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IYebF8y7na8"
      },
      "source": [
        "##Add obstacles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LXFwF8oUKwc0"
      },
      "outputs": [],
      "source": [
        "from pupperv3_mjx import obstacles\n",
        "import importlib\n",
        "importlib.reload(obstacles)\n",
        "\n",
        "tree = obstacles.add_boxes_to_model(\n",
        "    tree,\n",
        "    n_boxes=CONFIG.training.n_obstacles,\n",
        "    x_range=CONFIG.training.obstacle_x_range,\n",
        "    y_range=CONFIG.training.obstacle_y_range,\n",
        "    height=CONFIG.training.obstacle_height,\n",
        "    # depth=CONFIG.training.obstacle_depth,\n",
        "    length=CONFIG.training.obstacle_length\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijJ3bxAGbZcn"
      },
      "source": [
        "## Add height field ground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIpy5gLTygZQ"
      },
      "outputs": [],
      "source": [
        "if CONFIG.training.height_field_random or CONFIG.training.height_field_steps:\n",
        "  if CONFIG.training.height_field_random:\n",
        "    # Height field with 8cm resolution\n",
        "    noise = np.array(jax.random.uniform(jax.random.PRNGKey(0), (CONFIG.training.height_field_grid_size, CONFIG.training.height_field_grid_size)))\n",
        "\n",
        "    # Height field with 1m resolution\n",
        "    area_noise = jax.random.uniform(jax.random.PRNGKey(1),\n",
        "    (1*int(CONFIG.training.height_field_grid_size//CONFIG.training.height_field_radius_x),\n",
        "      1*int(CONFIG.training.height_field_grid_size//CONFIG.training.height_field_radius_y)))\n",
        "    upscaled_area_noise = np.array(jax.image.resize(image=area_noise,\n",
        "                                                  shape=(CONFIG.training.height_field_grid_size, CONFIG.training.height_field_grid_size),\n",
        "                                                  method=\"nearest\"))\n",
        "\n",
        "    # Height field where\n",
        "    scaled_noise = noise * upscaled_area_noise\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    axs[0].imshow(noise, cmap='viridis')\n",
        "    axs[0].set_title('High resolution base noise')\n",
        "    axs[1].imshow(area_noise, cmap='viridis')\n",
        "    axs[1].set_title('Low resolution noise to make discrete areas')\n",
        "    axs[2].imshow(scaled_noise, cmap='viridis')\n",
        "    axs[2].set_title('Final height field noise')\n",
        "\n",
        "  if CONFIG.training.height_field_steps:\n",
        "    # steps with 24cm width\n",
        "    steps = jax.random.uniform(jax.random.PRNGKey(0),\n",
        "                                (CONFIG.training.height_field_grid_size//CONFIG.training.height_field_step_size,\n",
        "                                 CONFIG.training.height_field_grid_size//CONFIG.training.height_field_step_size))\n",
        "    scaled_noise = np.array(jax.image.resize(image=steps,\n",
        "                                    shape=(CONFIG.training.height_field_grid_size, CONFIG.training.height_field_grid_size),\n",
        "                                    method=\"nearest\"))\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    axs[0].imshow(steps, cmap='viridis')\n",
        "    axs[0].set_title('Low resolution step heights')\n",
        "    axs[1].imshow(scaled_noise, cmap='viridis')\n",
        "    axs[1].set_title('Final height field noise')\n",
        "\n",
        "\n",
        "  root = tree.getroot()\n",
        "  worldbody = root.find(\"worldbody\")\n",
        "  asset = root.find(\"asset\")\n",
        "\n",
        "  ET.SubElement(\n",
        "      asset,\n",
        "      \"hfield\",\n",
        "      name=\"hfield_geom\",\n",
        "      # pos=\"0 0 0\",\n",
        "      nrow=f\"{CONFIG.training.height_field_grid_size}\",\n",
        "      ncol=f\"{CONFIG.training.height_field_grid_size}\",\n",
        "      elevation = ' '.join(scaled_noise.astype(str).flatten().tolist()),\n",
        "      size=f\"{CONFIG.training.height_field_radius_x} {CONFIG.training.height_field_radius_y} {CONFIG.training.height_field_elevation_z} {CONFIG.training.height_field_base_z}\",\n",
        "  )\n",
        "\n",
        "  ET.SubElement(\n",
        "      worldbody,\n",
        "      \"geom\",\n",
        "      name=\"hfield_floor\",\n",
        "      type=\"hfield\",\n",
        "      hfield=\"hfield_geom\",\n",
        "      # rgba=\"0.1 0.5 0.8 1\",\n",
        "      material=\"grid\",\n",
        "      conaffinity=\"1\",\n",
        "      contype=\"1\",\n",
        "      condim=\"3\",\n",
        "      group=CONFIG.training.height_field_group,\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs8tulPnA_WX"
      },
      "source": [
        "## Write new model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVg1ha0SA-zT"
      },
      "outputs": [],
      "source": [
        "with open(CONFIG.simulation.model_path,'w+') as file:\n",
        "  tree.write(file, encoding='unicode')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t9wIBXHfELi"
      },
      "source": [
        "# Benchmark pupper model\n",
        "\n",
        "A100 GPU\n",
        "* ~2M physics steps/s with 20 max collision pairs/pts and flat ground.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fzOj6H6frnN"
      },
      "outputs": [],
      "source": [
        "from mujoco.mjx import benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6FWHSo_omSR"
      },
      "outputs": [],
      "source": [
        "# Using `with` syntax\n",
        "# bfloat16: 2M\n",
        "# tensorfloat32: 1.8M\n",
        "# float32: 1.8M\n",
        "# with jax.default_matmul_precision(\"bfloat16\"):\n",
        "\n",
        "# Using `jax.config.update` method\n",
        "# high: 1.8M\n",
        "# bfloat16: 2.05M\n",
        "# tensorfloat32: 1.8M\n",
        "# default: 2.05M\n",
        "jax.config.update('jax_default_matmul_precision', 'high')\n",
        "sys = mjcf.load(CONFIG.simulation.model_path)\n",
        "jit_time, run_time, steps = benchmark(sys.mj_model, batch_size=CONFIG.training.ppo.num_envs)\n",
        "physics_steps_per_sec = steps / run_time\n",
        "print('Steps per sec: ', physics_steps_per_sec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efnxNOnpQFuC"
      },
      "source": [
        "# Pupper V3 Env\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-PvI2gexJQ3"
      },
      "source": [
        "## Create Pupper V3 Env\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y79PoJOCIl-O"
      },
      "outputs": [],
      "source": [
        "from pupperv3_mjx import environment\n",
        "import importlib\n",
        "importlib.reload(environment)\n",
        "\n",
        "envs.register_environment('pupper', environment.PupperV3Env)\n",
        "\n",
        "env_kwargs = dict(path=CONFIG.simulation.model_path,\n",
        "                  action_scale=CONFIG.policy.action_scale,\n",
        "                  observation_history=CONFIG.policy.observation_history,\n",
        "                  joint_lower_limits=joint_lower_limits,\n",
        "                  joint_upper_limits=joint_upper_limits,\n",
        "                  dof_damping=CONFIG.training.dof_damping,\n",
        "                  position_control_kp=CONFIG.training.position_control_kp,\n",
        "                  foot_site_names=CONFIG.simulation.foot_site_names,\n",
        "                  torso_name=CONFIG.simulation.torso_name,\n",
        "                  upper_leg_body_names=CONFIG.simulation.upper_leg_body_names,\n",
        "                  lower_leg_body_names=CONFIG.simulation.lower_leg_body_names,\n",
        "                  resample_velocity_step=CONFIG.training.resample_velocity_step,\n",
        "                  linear_velocity_x_range=CONFIG.training.lin_vel_x_range,\n",
        "                  linear_velocity_y_range=CONFIG.training.lin_vel_y_range,\n",
        "                  angular_velocity_range=CONFIG.training.ang_vel_yaw_range,\n",
        "                  zero_command_probability=CONFIG.training.zero_command_probability,\n",
        "                  stand_still_command_threshold=CONFIG.training.stand_still_command_threshold,\n",
        "                  maximum_pitch_command=CONFIG.training.maximum_pitch_command,\n",
        "                  maximum_roll_command=CONFIG.training.maximum_roll_command,\n",
        "                  start_position_config=CONFIG.training.start_position_config,\n",
        "                  default_pose=CONFIG.training.default_pose,\n",
        "                  desired_abduction_angles=CONFIG.training.desired_abduction_angles,\n",
        "                  reward_config=CONFIG.reward,\n",
        "                  angular_velocity_noise=CONFIG.training.angular_velocity_noise,\n",
        "                  gravity_noise=CONFIG.training.gravity_noise,\n",
        "                  motor_angle_noise=CONFIG.training.motor_angle_noise,\n",
        "                  last_action_noise=CONFIG.training.last_action_noise,\n",
        "                  kick_vel = CONFIG.training.kick_vel,\n",
        "                  kick_probability = CONFIG.training.kick_probability,\n",
        "                  terminal_body_z=CONFIG.training.terminal_body_z,\n",
        "                  early_termination_step_threshold=CONFIG.training.early_termination_step_threshold,\n",
        "                  terminal_body_angle=CONFIG.training.terminal_body_angle,\n",
        "                  foot_radius=CONFIG.simulation.foot_radius,\n",
        "                  environment_timestep=CONFIG.training.environment_dt,\n",
        "                  physics_timestep=CONFIG.simulation.physics_dt,\n",
        "                  latency_distribution=CONFIG.training.latency_distribution,\n",
        "                  imu_latency_distribution=CONFIG.training.imu_latency_distribution,\n",
        "                  desired_world_z_in_body_frame=jp.array(CONFIG.training.desired_world_z_in_body_frame),\n",
        "                  use_imu=CONFIG.policy.use_imu,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi_yrcz-Qp3W"
      },
      "outputs": [],
      "source": [
        "env_name = 'pupper'\n",
        "env = envs.get_environment(env_name, **env_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVsJY3ML2Jb6"
      },
      "source": [
        "## Visualize single env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e35uuUorihI"
      },
      "outputs": [],
      "source": [
        "visualization_env = envs.get_environment(env_name, **env_kwargs)\n",
        "\n",
        "# initialize the state\n",
        "rng = jax.random.PRNGKey(1)\n",
        "\n",
        "jit_reset = jax.jit(visualization_env.reset)\n",
        "jit_step = jax.jit(visualization_env.step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2ckSVIktgj8"
      },
      "outputs": [],
      "source": [
        "state = jit_reset(rng)\n",
        "state.info['command'] = jp.array([0, 0, 0])\n",
        "\n",
        "rollout = [state.pipeline_state]\n",
        "states = [state]\n",
        "\n",
        "# grab a trajectory\n",
        "n_steps = 200\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps):\n",
        "  act_rng, rng = jax.random.split(rng)\n",
        "  ctrl = jp.ones(shape=(sys.nu,)) * jp.sin(i * visualization_env.dt * 2 * 3.14 * 2) * 0.25\n",
        "  # ctrl = jp.array(np.random.uniform(low=-1.0, high=1.0, size=sys.nu))*0.5\n",
        "  state = jit_step(state, ctrl)\n",
        "  rollout.append(state.pipeline_state)\n",
        "  states.append(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLJhubCArl36"
      },
      "outputs": [],
      "source": [
        "media.show_video(\n",
        "    visualization_env.render(rollout[::render_every], camera='tracking_cam'),\n",
        "    fps=1.0 / visualization_env.dt / render_every)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGM_fwFNPhZk"
      },
      "outputs": [],
      "source": [
        "states[-1].info[\"rewards\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVRgHQ09WBZb"
      },
      "outputs": [],
      "source": [
        "# prompt: using plotly, plot states[i].pipeline_state.qpos where i is the x axis\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "x_axis = list(range(len(states)))\n",
        "y_axis = [state.pipeline_state.qpos for state in states]\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "for i in range(y_axis[0].shape[0]):\n",
        "  fig.add_trace(go.Scatter(x=x_axis, y=[qpos[i] for qpos in y_axis],\n",
        "                         mode='lines',\n",
        "                         name=f'Joint {i}'))\n",
        "\n",
        "fig.update_layout(title='Joint Positions over Time',\n",
        "                  xaxis_title='Timestep',\n",
        "                  yaxis_title='Joint Position')\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQh-ufdL2OrV"
      },
      "source": [
        "## Benchmark env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otfRlWAs6b9L"
      },
      "source": [
        "L4 GPU: Typically 400,000 env steps/sec on flat ground.\n",
        "* 366,000 step/s with 0 obstacles and 8, 8\n",
        "* 181,000 step/s with hfield, no box body\n",
        "* 121,000 step/s with 20 ledges and 8, 8\n",
        "* 105,000 step/s with 100 ledges and 8, 8\n",
        "* 12,000 with 300 obstacles\n",
        "* 24,000 for hfield with 8, 8!! SO SLOW!\n",
        "* 24,700 for hfield with 5, 4!! SLOW!\n",
        "\n",
        "* 60,000 step/s with 20 ledges and 15, 15 for contact pts and geom pairs\n",
        "\n",
        "A100 GPU:\n",
        "* 760,000 step/s with 0 ledges, 8, 8\n",
        "* 545,000 step/s with 2 ledges, 8, 8\n",
        "* 330,000 step/s with 10 ledges, 8, 8\n",
        "* 87,000 step/s with hfield, 5, 4 (wrong ?)\n",
        "* 250,000 step/s with hfield, 5, 4\n",
        "* 66,000 with hfield, 8, 8, elliptic, impratio=100, 10 iterations, 50 ls iterations\n",
        "* 370,000 with hfield, 8, 8, pyramidal, impratio=10, iterations=1, ls_iterations=5,\n",
        "* 370,000 with ^ except for 128 size hfield\n",
        "* 200,000 with ^ except 10 history and 256, 256, 256, 256 policy\n",
        "* 280,000 with ^ except 2 history, 256, 128, 128, 128 policy\n",
        "* 264,000 with hfield, more contacts, 1024, 512, 512, 512 policy\n",
        "* 265,000 with hfield, more contacts, 512, 256, 128 policy 3 legged\n",
        "* These tests do not include policy time!\n",
        "\n",
        "H100 GPU:\n",
        "* 121,728 step/s with hfield, 5, 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJgYXFlgtsEo"
      },
      "outputs": [],
      "source": [
        "# batch_size = CONFIG.training.ppo.num_envs\n",
        "# unroll_steps = 1\n",
        "# nstep = CONFIG.training.ppo.episode_length\n",
        "# from mujoco.mjx._src import forward\n",
        "# from mujoco.mjx._src import io\n",
        "\n",
        "\n",
        "# def _measure(fn, *args):\n",
        "#   \"\"\"Reports jit time and op time for a function.\"\"\"\n",
        "\n",
        "#   beg = time.perf_counter()\n",
        "#   compiled_fn = fn.lower(*args).compile()\n",
        "#   end = time.perf_counter()\n",
        "#   jit_time = end - beg\n",
        "\n",
        "#   beg = time.perf_counter()\n",
        "#   result = compiled_fn(*args)\n",
        "#   jax.block_until_ready(result)\n",
        "#   end = time.perf_counter()\n",
        "#   run_time = end - beg\n",
        "\n",
        "#   return jit_time, run_time\n",
        "\n",
        "# @jax.pmap\n",
        "# def init(key):\n",
        "#   key = jax.random.split(key, batch_size // jax.device_count())\n",
        "\n",
        "#   @jax.vmap\n",
        "#   def random_init(key):\n",
        "#     return visualization_env.reset(key)\n",
        "\n",
        "#   return random_init(key)\n",
        "\n",
        "# key = jax.random.split(jax.random.key(0), jax.device_count())\n",
        "# d = init(key)\n",
        "# jax.block_until_ready(d)\n",
        "\n",
        "\n",
        "# @jax.pmap\n",
        "# def unroll(d):\n",
        "#   @jax.vmap\n",
        "#   def step(carry, _):\n",
        "#     d, rng = carry\n",
        "#     rng, key = jax.random.split(rng)\n",
        "#     ctrl = jax.random.uniform(key, shape=(visualization_env.sys.nu,))\n",
        "#     d = visualization_env.step(d, ctrl)\n",
        "#     return (d, rng), None\n",
        "\n",
        "#   rng = jax.random.split(jax.random.PRNGKey(0), batch_size)\n",
        "#   d, _ = jax.lax.scan(step, (d, rng), None, length=nstep, unroll=unroll_steps)\n",
        "\n",
        "#   return d\n",
        "\n",
        "\n",
        "# jit_time, run_time = _measure(unroll, d)\n",
        "# steps = nstep * batch_size\n",
        "\n",
        "# print(jit_time, steps / run_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxaNFP9mA23H"
      },
      "source": [
        "# Train Policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6ty6pabkmW3"
      },
      "source": [
        "Expected reward:\n",
        "* episode_length = 1000 -> 30+\n",
        "* episode_length = 500  -> 15+\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHJCbESGA7Rk"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from pupperv3_mjx import utils\n",
        "import importlib\n",
        "\n",
        "importlib.reload(utils)\n",
        "\n",
        "train_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "ENTITY = None\n",
        "wandb.init(entity=ENTITY,\n",
        "           project=\"pupperv3-mjx-rl\",\n",
        "           config=CONFIG.to_dict(),\n",
        "           save_code=True,\n",
        "           settings={\n",
        "           \"_service_wait\": 90,\n",
        "           \"init_timeout\": 90})\n",
        "\n",
        "try:\n",
        "    wandb.run.summary[\"benchmark_physics_steps_per_sec\"] = physics_steps_per_sec\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Save and reload params.\n",
        "output_folder = f\"output_{wandb.run.name}\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Create and JIT reset and step functions for use in in-training policy\n",
        "# video creation if they don't already exist from a previous step\n",
        "if (\"jit_reset\" in globals() or \"jit_reset\" in locals()) and (\n",
        "    \"jit_step\" in globals() or \"jit_step\" in locals()\n",
        "):\n",
        "    print(\"JIT'd step and reset functions already defined. \" \"Using them for policy visualization.\")\n",
        "else:\n",
        "    print(\"Creating and JIT'ing step and reset functions\")\n",
        "    policy_viz_env = envs.get_environment(env_name, **env_kwargs)\n",
        "    jit_reset = jax.jit(policy_viz_env.reset)\n",
        "    jit_step = jax.jit(policy_viz_env.step)\n",
        "\n",
        "\n",
        "make_networks_factory = functools.partial(\n",
        "    ppo_networks.make_ppo_networks,\n",
        "    policy_hidden_layer_sizes=CONFIG.policy.hidden_layer_sizes,\n",
        "    activation=utils.activation_fn_map(CONFIG.policy.activation)\n",
        ")\n",
        "train_fn = functools.partial(\n",
        "    ppo.train,\n",
        "    **(CONFIG.training.ppo.to_dict()),\n",
        "    network_factory=make_networks_factory,\n",
        "    randomization_fn=functools.partial(\n",
        "        domain_randomization.domain_randomize,\n",
        "        friction_range=CONFIG.training.friction_range,\n",
        "        kp_multiplier_range=CONFIG.training.position_control_kp_multiplier_range,\n",
        "        kd_multiplier_range=CONFIG.training.position_control_kd_multiplier_range,\n",
        "        body_com_x_shift_range=CONFIG.training.body_com_x_shift_range,\n",
        "        body_com_y_shift_range=CONFIG.training.body_com_y_shift_range,\n",
        "        body_com_z_shift_range=CONFIG.training.body_com_z_shift_range,\n",
        "        body_mass_scale_range=CONFIG.training.body_mass_scale_range,\n",
        "        body_inertia_scale_range=CONFIG.training.body_inertia_scale_range,\n",
        "    ),\n",
        "    seed=28,\n",
        ")\n",
        "\n",
        "x_data = []\n",
        "y_data = []\n",
        "ydataerr = []\n",
        "times = [datetime.now()]\n",
        "\n",
        "env = envs.get_environment(env_name, **env_kwargs)\n",
        "eval_env = envs.get_environment(env_name, **env_kwargs)\n",
        "\n",
        "def policy_params_fn(current_step, make_policy, params):\n",
        "    utils.visualize_policy(current_step=current_step,\n",
        "                           make_policy=make_policy,\n",
        "                           params=params,\n",
        "                           eval_env=eval_env,\n",
        "                           jit_step=jit_step,\n",
        "                           jit_reset=jit_reset,\n",
        "                           output_folder=output_folder)\n",
        "    utils.save_checkpoint(current_step=current_step,\n",
        "                          make_policy=make_policy,\n",
        "                          params=params,\n",
        "                          checkpoint_path=output_folder)\n",
        "\n",
        "from pathlib import Path\n",
        "checkpoint_kwargs = {}\n",
        "if CONFIG.training.checkpoint_run_number is not None:\n",
        "  utils.download_checkpoint(entity_name=ENTITY,\n",
        "                            project_name=\"pupperv3-mjx-rl\",\n",
        "                            run_number=CONFIG.training.checkpoint_run_number,\n",
        "                            save_path=\"checkpoint\")\n",
        "  checkpoint_kwargs[\"restore_checkpoint_path\"]=Path(\"checkpoint\").resolve()\n",
        "\n",
        "make_inference_fn, params, _ = train_fn(\n",
        "    environment=env,\n",
        "    progress_fn=functools.partial(\n",
        "        utils.progress,\n",
        "        times=times,\n",
        "        x_data=x_data,\n",
        "        y_data=y_data,\n",
        "        ydataerr=ydataerr,\n",
        "        num_timesteps=CONFIG.training.ppo.num_timesteps,\n",
        "        min_y=0,\n",
        "        max_y=40,\n",
        "    ),\n",
        "    eval_env=eval_env,\n",
        "    policy_params_fn=policy_params_fn,\n",
        "    **checkpoint_kwargs\n",
        ")\n",
        "\n",
        "print(f\"time to jit: {times[1] - times[0]}\")\n",
        "print(f\"time to train: {times[-1] - times[1]}\")\n",
        "wandb.run.summary[\"time_to_jit\"] = (times[1] - times[0]).total_seconds()\n",
        "wandb.run.summary[\"time_to_train\"] = (times[-1] - times[1]).total_seconds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yge-CGP5JoO"
      },
      "outputs": [],
      "source": [
        "model_path = os.path.join(output_folder, f'mjx_params_{train_datetime}')\n",
        "model.save_params(model_path, params)\n",
        "# params = model.load_params(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SV7kSSwNlLW"
      },
      "outputs": [],
      "source": [
        "inference_fn = make_inference_fn(params)\n",
        "jit_inference_fn = jax.jit(inference_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L01IrN4oCIkC"
      },
      "source": [
        "# Visualize Policy\n",
        "\n",
        "For the Barkour Quadruped, the joystick commands can be set through `x_vel`, `y_vel`, and `ang_vel`. `x_vel` and `y_vel` define the linear forward and sideways velocities with respect to the quadruped torso. `ang_vel` defines the angular velocity of the torso in the z direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTbpEtXnEecd"
      },
      "outputs": [],
      "source": [
        "eval_env = envs.get_environment(env_name, **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRRN-8L-BivZ"
      },
      "outputs": [],
      "source": [
        "# @markdown Commands **only used for Pupper Env**:\n",
        "x_vel = 0.3  #@param {type: \"number\"}\n",
        "y_vel = -0.0  #@param {type: \"number\"}\n",
        "ang_vel = 0.0  #@param {type: \"number\"}\n",
        "pitch_degs = 0 #@param {type: \"number\"}\n",
        "\n",
        "the_command = jp.array([x_vel, y_vel, ang_vel])\n",
        "\n",
        "# initialize the state\n",
        "rng = jax.random.PRNGKey(0)\n",
        "state = jit_reset(rng)\n",
        "state.info['command'] = the_command\n",
        "pitch_rad = pitch_degs * 3.14/180.0\n",
        "state.info['desired_world_z_in_body_frame'] = jp.array([jp.sin(pitch_rad),\n",
        "                                                        0.0,\n",
        "                                                        jp.cos(pitch_rad)])\n",
        "rollout = [state.pipeline_state]\n",
        "\n",
        "# grab a trajectory\n",
        "n_steps = 200\n",
        "render_every = 2\n",
        "ctrls = []\n",
        "\n",
        "for i in range(n_steps):\n",
        "  act_rng, rng = jax.random.split(rng)\n",
        "\n",
        "  ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
        "  state = jit_step(state, ctrl)\n",
        "  rollout.append(state.pipeline_state)\n",
        "  ctrls.append(ctrl)\n",
        "\n",
        "media.show_video(\n",
        "    eval_env.render(rollout[::render_every], camera='tracking_cam'),\n",
        "    fps=1.0 / eval_env.dt / render_every)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBeQeuPkWlhi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pupperv3_mjx import plotting\n",
        "plotting.plot_multi_series(data=np.array(ctrls), dt=0.02, display_axes=[0,1,2], title=\"Policy output\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MTsReLS43xD"
      },
      "outputs": [],
      "source": [
        "torques = jp.array([rollout[i].qfrc_actuator[6:] for i in range(len(rollout))]) # ignore world-body joint\n",
        "plotting.plot_multi_series(data=torques, dt=0.02, display_axes=[0, 1, 2], title=\"Joint torques\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eImaSgpZCP3j"
      },
      "outputs": [],
      "source": [
        "joint_pos = jp.array([rollout[i].q[7:] for i in range(len(rollout))]) # ignore world-body joint\n",
        "plotting.plot_multi_series(data=joint_pos, dt=0.02, display_axes=[0, 1, 2], title=\"Joint Positions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mN8Y1QChNRH"
      },
      "outputs": [],
      "source": [
        "world_vel = jp.array([rollout[i].qvel[:3] for i in range(len(rollout))])\n",
        "plotting.plot_multi_series(data=world_vel, dt=0.02, display_axes=[0, 1, 2], title=\"World Velocity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD6H6WD0915X"
      },
      "source": [
        "We can also render the rollout using the Brax renderer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7jqv08X95u4"
      },
      "outputs": [],
      "source": [
        "# HTML(html.render(eval_env.sys.replace(dt=eval_env.dt), rollout))\n",
        "# something removed changed in the api\n",
        "# HTML(html.render(eval_env.sys, rollout))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDo1W2nkjeG6"
      },
      "source": [
        "# Export Policy for neural_controller"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-gvFL6zi5CY"
      },
      "source": [
        "After running the following cells, open the files tab on the left and download policy.json."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWFgxqB71E4O"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pupperv3_mjx import export\n",
        "params_rtneural = export.convert_params(jax.block_until_ready(params),\n",
        "                                        activation=CONFIG.policy.activation,\n",
        "                                        action_scale=CONFIG.policy.action_scale,\n",
        "                                        kp=CONFIG.training.position_control_kp,\n",
        "                                        kd=CONFIG.training.dof_damping,\n",
        "                                        default_pose=CONFIG.training.default_pose,\n",
        "                                        joint_upper_limits=CONFIG.simulation.joint_upper_limits,\n",
        "                                        joint_lower_limits=CONFIG.simulation.joint_lower_limits,\n",
        "                                        use_imu=CONFIG.policy.use_imu,\n",
        "                                        observation_history=CONFIG.policy.observation_history,\n",
        "                                        maximum_pitch_command=CONFIG.training.maximum_pitch_command,\n",
        "                                        maximum_roll_command=CONFIG.training.maximum_roll_command,\n",
        "                                        final_activation=\"tanh\",\n",
        "                                        )\n",
        "\n",
        "name = f\"policy_{wandb.run.name}_max_reward_{y_data[-1]:0.2f}.json\"\n",
        "saved_policy_filename = os.path.join(output_folder, name)\n",
        "with open(saved_policy_filename, \"w\") as f:\n",
        "  json.dump(params_rtneural, f)\n",
        "\n",
        "wandb.log_model(path=saved_policy_filename, name=name)\n",
        "wandb.log_model(path=model_path, name=f\"mjx_policy_network_{wandb.run.name}.pt\")\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMnVQjByig7K"
      },
      "source": [
        "# Shutdown the runtime to save money"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SAVE POLICY TO GOOGLE DRIVE\n",
        "# ============================================================================\n",
        "# Copy this entire cell into your Colab notebook as the LAST cell.\n",
        "# It will:\n",
        "#   1. Mount Google Drive\n",
        "#   2. Copy the trained policy folder to Google Drive\n",
        "#   3. Unassign the runtime to save credits\n",
        "#\n",
        "# Prerequisites:\n",
        "#   - 'output_folder' must be defined from the training cell (line ~2027):\n",
        "#       output_folder = f\"output_{wandb.run.name}\"\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from google.colab import runtime\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define Output Paths\n",
        "# 'output_folder' should be defined in the previous cells of the notebook\n",
        "source_path = output_folder\n",
        "\n",
        "# Define where in Drive to save. Change 'Pupper_Policies' if you prefer a different folder.\n",
        "drive_base_dir = '/content/drive/My Drive/Pupper_Policies'\n",
        "destination_path = os.path.join(drive_base_dir, os.path.basename(source_path))\n",
        "\n",
        "print(f\"Source: {source_path}\")\n",
        "print(f\"Destination: {destination_path}\")\n",
        "\n",
        "# 3. Copy files\n",
        "if os.path.exists(destination_path):\n",
        "    print(f\"Destination folder {destination_path} already exists. Removing it to overwrite...\")\n",
        "    shutil.rmtree(destination_path)\n",
        "\n",
        "print(\"Copying policy files to Google Drive...\")\n",
        "shutil.copytree(source_path, destination_path)\n",
        "print(\"Successfully saved policy to Google Drive!\")\n",
        "\n",
        "# 4. Unassign Runtime to save credits\n",
        "print(\"Unassigning runtime to save credits...\")\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "XWy-tNFFTpzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tSHUVwtnH7f"
      },
      "outputs": [],
      "source": [
        "# Give time for wandb to actually finish?\n",
        "import time\n",
        "time.sleep(10)\n",
        "\n",
        "# Helpful for overnight runs\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRPCRSTaZgYb"
      },
      "source": [
        "# Authors\n",
        "Google, Nathan, Gabrael"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "YvyGCsgSCxHQ",
        "d7y2mHlcs-uc",
        "ncR52Hvl3ZOn",
        "mi69q6o5X8qC",
        "6iKn6sSo7kSy",
        "-IYebF8y7na8",
        "ijJ3bxAGbZcn",
        "GQh-ufdL2OrV"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}